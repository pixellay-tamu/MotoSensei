# -*- coding: utf-8 -*-
"""tidal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vEvvrfjSOpsMffNYbD906mEpyQjq8BCm
"""

! pip install opendatasets

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import opendatasets as od
import pandas as pd
import os

od.download("https://www.kaggle.com/datasets/gpiosenka/car-parts-40-classes")

data = pd.read_csv("/content/car-parts-40-classes/car parts 50/car parts.csv")
data.head()



import numpy as np
import os
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

# Define image dimensions (should match preprocessing function)
IMG_HEIGHT = 224
IMG_WIDTH = 224
IMG_CHANNELS = 1 # Single color channel (grayscale)

# Function to load and preprocess images from a directory
def load_images_from_directory(directory):
    images = []
    labels = []
    for class_name in os.listdir(directory):
        class_dir = os.path.join(directory, class_name)
        if os.path.isdir(class_dir):
            for image_name in os.listdir(class_dir):
                image_path = os.path.join(class_dir, image_name)
                try:
                    # Load image
                    img = tf.io.read_file(image_path)
                    # Decode image
                    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)
                    # Resize image
                    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])
                    # Normalize pixel values to be between 0 and 1
                    img = img / 255.0
                    images.append(img.numpy())
                    labels.append(class_name)
                except Exception as e:
                    print(f"Error loading image {image_path}: {e}")
    return np.array(images), np.array(labels)

# Define the base directory
base_dir = "/content/car-parts-40-classes/car parts 50"

# Load and preprocess train images
train_images_array, train_labels_array = load_images_from_directory(os.path.join(base_dir, 'train'))
# Convert string labels to integer labels
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels_array)
# Convert integer labels to one-hot encoded labels
train_labels_one_hot = to_categorical(train_labels_encoded)


# Load and preprocess test images
test_images_array, test_labels_array = load_images_from_directory(os.path.join(base_dir, 'test'))
# Convert string labels to integer labels
test_labels_encoded = label_encoder.transform(test_labels_array) # Use the same encoder fitted on training data
# Convert integer labels to one-hot encoded labels
test_labels_one_hot = to_categorical(test_labels_encoded)


print("Train images shape:", train_images_array.shape)
print("Train labels shape:", train_labels_one_hot.shape)
print("Test images shape:", test_images_array.shape)
print("Test labels shape:", test_labels_one_hot.shape)

fig, axes = plt.subplots(1, 5, figsize=(10, 5))
for i in range(5):
    axes[i].imshow(train_images_array[i].reshape(224, 224), cmap='gray')
    axes[i].axis('off')
plt.show()

from tensorflow.keras import models, layers

model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, IMG_CHANNELS)))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(256, (3, 3), activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.3))

model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(50, activation='softmax'))

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_images_array, train_labels_one_hot, epochs=15,
                    validation_data=(test_images_array, test_labels_one_hot))

test_loss, test_acc = model.evaluate(test_images_array, test_labels_one_hot, verbose=2)
print(f'Test accuracy: {test_acc}')

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

plt.show()

from google.colab import files
from PIL import Image, ImageDraw
import io
import numpy as np
import tensorflow as tf

# Step 1: Upload car part image
uploaded = files.upload()

for filename in uploaded.keys():
    image = Image.open(io.BytesIO(uploaded[filename])).convert('RGB')
    image = image.resize((224, 224))  # match training input shape
    display(image)

# Step 2: Prepare image array
image_array = np.array(image) / 255.0
image_array = np.expand_dims(image_array, axis=0)

# Step 3: Get model confidence with the part
confidence_with = model.predict(image_array)[0].max()  # take highest class probability

# Step 4: Mask the suspected part region (for test â€” replace with part-specific logic)
masked_image = np.array(image)
masked_image[90:130, 100:140, :] = 0  # example mask block
masked_array = np.expand_dims(masked_image / 255.0, axis=0)

confidence_without = model.predict(masked_array)[0].max()

# Step 5: Compute likelihood that the part is the fault source
likelihood_issue = 1 - (confidence_without - confidence_with)
print(f"Confidence with part: {confidence_with:.3f}")
print(f"Confidence without part: {confidence_without:.3f}")
print(f"Likelihood this part is the source of the issue: {likelihood_issue:.3f}")

model.save("/content/gdrive/My Drive/lastModel.h5")